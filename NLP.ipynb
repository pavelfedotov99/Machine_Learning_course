{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "956VvJPL_vlr"
      },
      "source": [
        "Import of the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zXs29E4-tqm"
      },
      "source": [
        "import re\n",
        "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn93XXku_t-z"
      },
      "source": [
        "Reading the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWPNXIqf_ctL"
      },
      "source": [
        "alicetext = open(\"/content/Alice.txt\", 'r').read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADbRb95K_5zU"
      },
      "source": [
        "Task 1. \\\n",
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idx01kjk_nx7"
      },
      "source": [
        "# Removing all irrelevant characters\n",
        "alice = re.sub(\"<.*?>\", \" \", alicetext)\n",
        "alice = re.sub(\" +\", \" \", alice)\n",
        "alice = re.sub(r\"[^\\w\\s]\", \"\", alice)\n",
        "alice = re.sub(r\"\\d\", \"\", alice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1O_4hJf_oTU"
      },
      "source": [
        "# Tokenization\n",
        "tokens = WhitespaceTokenizer().tokenize(alice)[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DUq4oWzATEc"
      },
      "source": [
        "#Lower case\n",
        "tokens = [token.lower() for token in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSJGnh3TAU6z",
        "outputId": "c5aab9f6-e042-4dfc-9ff6-b3cf090db7e8"
      },
      "source": [
        "#Removing stop-words\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")\n",
        "tokens = [token for token in tokens if token not in stop_words]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-_4sKvfAfbT",
        "outputId": "9d0e6583-b0f5-4cc4-e646-2930091bf959"
      },
      "source": [
        "# Lemmatization\n",
        "nltk.download(\"wordnet\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = [lemmatizer.lemmatize(token) for token in tokens][:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufpk94pTBB9a"
      },
      "source": [
        "Task 2. \\\n",
        "Top 10 most importantwords from each chapter in the text (not \"Alice\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3-BKXowAuW8"
      },
      "source": [
        "my_string = ' '.join([str(elem) for elem in tokens]) \n",
        "chapter_texts = []\n",
        "for chap_n in range(13, 25):\n",
        "  stroka = my_string.split(\"chapter\",chap_n)[chap_n]\n",
        "  chapter_texts.append([stroka])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5MsF6wAAu4Y",
        "outputId": "e3be5214-2110-4417-b580-362761f2b898"
      },
      "source": [
        "for i, chap in enumerate(chapter_texts):\n",
        "  tfidf = TfidfVectorizer()\n",
        "  X = tfidf.fit_transform(chap)\n",
        "  feature_names = np.array(tfidf.get_feature_names())\n",
        "  tfidf_sorting = np.argsort(X.toarray()).flatten()[::-1]\n",
        "  n = 11\n",
        "  topn = feature_names[tfidf_sorting][:n]\n",
        "  words = []\n",
        "  for el in topn:\n",
        "    if el == 'alice':\n",
        "      continue\n",
        "    else:\n",
        "      words.append(el)\n",
        "  print('\\nIn chapter {} Top-10 most important words are:\\n{}'.format(i+1, words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "In chapter 1 Top-10 most important words are:\n",
            "['queen', 'one', 'would', 'know', 'little', 'like', 'said', 'went', 'thing', 'thought']\n",
            "\n",
            "In chapter 2 Top-10 most important words are:\n",
            "['thing', 'queen', 'like', 'went', 'said', 'thought', 'would', 'little', 'one', 'know']\n",
            "\n",
            "In chapter 3 Top-10 most important words are:\n",
            "['little', 'thing', 'king', 'one', 'would', 'said', 'went', 'queen', 'like', 'know']\n",
            "\n",
            "In chapter 4 Top-10 most important words are:\n",
            "['said', 'turtle', 'would', 'like', 'king', 'went', 'queen', 'one', 'know', 'little']\n",
            "\n",
            "In chapter 5 Top-10 most important words are:\n",
            "['turtle', 'little', 'know', 'king', 'one', 'would', 'went', 'said', 'queen', 'hatter']\n",
            "\n",
            "In chapter 6 Top-10 most important words are:\n",
            "['said', 'little', 'mock', 'king', 'went', 'one', 'hatter', 'turtle', 'queen', 'know']\n",
            "\n",
            "In chapter 7 Top-10 most important words are:\n",
            "['went', 'turtle', 'king', 'know', 'queen', 'said', 'mock', 'one', 'gryphon', 'hatter']\n",
            "\n",
            "In chapter 8 Top-10 most important words are:\n",
            "['one', 'queen', 'went', 'turtle', 'little', 'would', 'gryphon', 'said', 'mock', 'king']\n",
            "\n",
            "In chapter 9 Top-10 most important words are:\n",
            "['king', 'turtle', 'would', 'one', 'little', 'gryphon', 'queen', 'said', 'mock', 'know']\n",
            "\n",
            "In chapter 10 Top-10 most important words are:\n",
            "['gryphon', 'said', 'little', 'would', 'mock', 'one', 'hatter', 'turtle', 'king', 'voice']\n",
            "\n",
            "In chapter 11 Top-10 most important words are:\n",
            "['king', 'queen', 'would', 'hatter', 'jury', 'little', 'rabbit', 'one', 'said', 'court']\n",
            "\n",
            "In chapter 12 Top-10 most important words are:\n",
            "['queen', 'would', 'rabbit', 'head', 'little', 'jury', 'white', 'said', 'know', 'king']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR7lBlrfDGkb"
      },
      "source": [
        "Task 3.\n",
        "Finding the Top 10 most used verbs in sentences with Alice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E64QO7coC_Z9"
      },
      "source": [
        "verbs_csv = pd.read_csv(\"/content/englishverbs.csv\",sep = ';',encoding= 'unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "OF_Zlk_6EwWa",
        "outputId": "88766bd7-76b3-487a-c9c3-359cb31f2686"
      },
      "source": [
        "verbs_csv.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Num</th>\n",
              "      <th>Base Form</th>\n",
              "      <th>Past Form</th>\n",
              "      <th>Past Participle Form</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>abash</td>\n",
              "      <td>abashed</td>\n",
              "      <td>abashed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>abate</td>\n",
              "      <td>abated</td>\n",
              "      <td>abated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>abide</td>\n",
              "      <td>abode</td>\n",
              "      <td>abode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>absorb</td>\n",
              "      <td>absorbed</td>\n",
              "      <td>absorbed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>accept</td>\n",
              "      <td>accepted</td>\n",
              "      <td>accepted</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Num Base Form Past Form Past Participle Form\n",
              "0    1     abash   abashed              abashed\n",
              "1    2     abate    abated               abated\n",
              "2    3     abide     abode                abode\n",
              "3    4    absorb  absorbed             absorbed\n",
              "4    5    accept  accepted             accepted"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9dsaE_zExXO"
      },
      "source": [
        "#Creating a list of the verbs\n",
        "verbs = []\n",
        "x = verbs_csv['Base Form'].values\n",
        "y = verbs_csv['Past Form'].values\n",
        "z = verbs_csv['Past Participle Form'].values\n",
        "verbs.append(x)\n",
        "verbs.append(y)\n",
        "verbs.append(z)\n",
        "verbs = np.reshape(verbs, (1, -1))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDkaf-mwsUQI"
      },
      "source": [
        "Removing all irrelevant characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g03HHPEbsVL1"
      },
      "source": [
        "text = re.sub(\"<*>|\\n|\\n\\n\", \" \", alicetext)\n",
        "text = re.sub(\" +\", \" \", text)\n",
        "text = re.sub(r\"[^\\w\\s\\.]\", \"\", text)\n",
        "text = re.sub(r\"\\d\", \"\", text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edK_sgb_sdsN"
      },
      "source": [
        "#Tokenization\n",
        "tokens = re.split(r'[.|;|?|!|\"]', text)\n",
        "#Lower case\n",
        "tokens = [token.lower() for token in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeXASFKbsi5u"
      },
      "source": [
        "Finding all the senteces with the name Alice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI7W1fClsg7W"
      },
      "source": [
        "verbs_freq = dict()\n",
        "verbs = list(verbs)\n",
        "for sent in tokens:\n",
        "  x = sent.find('alice')\n",
        "  if 'alice' in sent:\n",
        "    sentence = WhitespaceTokenizer().tokenize(sent)[:]\n",
        "    # Update the frequency of the verbs\n",
        "    for w in sentence:\n",
        "      if w in verbs and len(verbs[verbs.index(w)]) == len(w):\n",
        "        if verbs_freq.get(w) is not None:\n",
        "          verbs_freq[w] += 1\n",
        "        else:\n",
        "          verbs_freq[w] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXpnLWcMtItE"
      },
      "source": [
        "Printing the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBpc4y0ns3eF",
        "outputId": "eecd32a2-c5f0-4d23-da3c-fd5bf9e6ca96"
      },
      "source": [
        "top10 = dict(sorted(verbs_freq.items(), key=lambda item: item[1], reverse=True)[:10])\n",
        "print('Top-10 verbs in sentences with Alice:')\n",
        "print(top10, '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top-10 verbs in sentences with Alice:\n",
            "{'said': 209, 'was': 184, 'had': 96, 'be': 81, 'thought': 61, 'like': 53, 'do': 47, 'went': 43, 'know': 41, 'see': 35} \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Oj-kYSJteb2"
      },
      "source": [
        "As we can see, the most frequent Alice actions are:\\\n",
        "To say\\\n",
        "To be\\\n",
        "To think\\\n",
        "To like\\\n",
        "To do\\\n",
        "To go\\\n",
        "To know\\\n",
        "To be able to(can)\\\n",
        "Would\\\n",
        "To see"
      ]
    }
  ]
}